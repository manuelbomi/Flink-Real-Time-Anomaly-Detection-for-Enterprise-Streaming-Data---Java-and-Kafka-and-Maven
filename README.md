# Flink Real-Time Anomaly Detection for Enterprise Streaming Data (Java + Kafka + Maven)

## Overview

Apache Flink is a high-performance, low-latency framework for real-time stream processing. It can ingest data from multiple sources including:


- Kafka, RabbitMQ, Kinesis – message queues

- Filesystems – HDFS, S3, local files

- Databases – JDBC, CDC (Change Data Capture)

- Socket streams – TCP/UDP

- Custom sources – sensors, IoT devices, or any real-time feed


This versatility is crucial in enterprise applications where data may originate from diverse systems.


Unlike Spark, Flink provides true stream-first processing (event-by-event) rather than micro-batching, which makes it ideal for low-latency real-time applications like anomaly detection.


This project demonstrates a real-time anomaly detection job that:

- Reads sensor data from a Kafka topic

- Computes sliding-window averages

- Flags anomalies where sensor readings deviate significantly from the window average
- 

The project is implemented in Java, packaged as a Maven JAR, and submitted to Flink. Running in Java provides:


- Lower latency and overhead than PyFlink

- Better integration with Flink’s checkpointing and fault tolerance

- Standardization for CI/CD and production deployments

---

## Project Directory Structure

```python
flink-anomaly/
├── pom.xml                                  # Maven build file with Flink dependencies
├── README.md                                # Project documentation
├── target/                                  # Generated by `mvn package`
│   ├── classes/                             # Compiled classes
│   └── flink-anomaly-1.0-SNAPSHOT.jar       # Executable JAR
├── src/
│   ├── main/
│   │   ├── java/
│   │   │   └── com/example/
│   │   │       └── AnomalyDetectionJob.java
│   │   └── resources/
│   │       └── application.properties       # Optional configs
│   └── test/
│       ├── java/
│       │   └── com/example/
│       │       └── AppTest.java             # Optional unit tests
│       └── resources/                        # Optional test configs
├── logs/                                    # Optional: store Flink job logs
└── scripts/
    └── run-flink-job.sh                      # Optional helper script


```

---

### Step 1: Install WSL and Ubuntu 22.04

Make sure your system has WSL2 and Ubuntu 22.04 installed:

```python
wsl --install -d Ubuntu-22.04
sudo apt update && sudo apt upgrade -y

```

Install Java 11 (required by Flink 1.19):

```python
sudo apt install openjdk-11-jdk -y
java -version

```

---

### Step 2: Install Flink and Kafka

#### Download Flink and extract:

```python
wget https://downloads.apache.org/flink/flink-1.19.1/flink-1.19.1-bin-scala_2.13.tgz
tar -xvzf flink-1.19.1-bin-scala_2.13.tgz
cd flink-1.19.1
```

#### Install Kafka (with Zookeeper) on WSL:

```python
wget https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz
tar -xvzf kafka_2.13-3.5.1.tgz
cd kafka_2.13-3.5.1

```

#### Start Zookeeper:

```python
bin/zookeeper-server-start.sh config/zookeeper.properties
```

#### Start Kafka broker:

```python
bin/kafka-server-start.sh config/server.properties
```

#### Create a topic for sensor data:

```python
bin/kafka-topics.sh --create --topic sensor-data_2 --bootstrap-server localhost:9092
```

#### Start a Kafka producer to push test data:

```python
bin/kafka-console-producer.sh --topic sensor-data_2 --bootstrap-server localhost:9092
```

---

## Step 3: Create a Maven Project

```python
sudo apt install maven -y
mvn archetype:generate \
  -DgroupId=com.example \
  -DartifactId=flink-anomaly \
  -DarchetypeArtifactId=maven-archetype-quickstart \
  -DinteractiveMode=false
cd flink-anomaly
```
---

### Step 3a: Add Java Source

Place AnomalyDetectionJob.java inside:

```python
src/main/java/com/example/
```

Confirm:

```python
pwd
# ~/flink-anomaly/src/main/java/com/example
```
---

### Step 3b: Add Flink Dependencies

Edit pom.xml to include:

```python
<properties>
    <maven.compiler.source>11</maven.compiler.source>
    <maven.compiler.target>11</maven.compiler.target>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java</artifactId>
        <version>1.19.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients</artifactId>
        <version>1.19.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-connector-kafka</artifactId>
        <version>3.2.0-1.19</version>
    </dependency>
</dependencies>

```

---

### Step 4: Build the Project

```python
mvn clean package
```

The JAR will appear in:

```python
target/flink-anomaly-1.0-SNAPSHOT.jar
```

---

### Step 5: Submit the Job to Flink

From your Flink directory:

```python
cp ~/flink-anomaly/target/flink-anomaly-1.0-SNAPSHOT.jar ~/flink-1.19.1/
cd ~/flink-1.19.1
./bin/flink run flink-anomaly-1.0-SNAPSHOT.jar

```
